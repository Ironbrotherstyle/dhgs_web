<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Decoupled Hybrid Gaussian Splatting for Driving Scene.">
  <!-- <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1"> -->
  <title>DHGS:Decoupled Hybrid Gaussian Splatting for Driving Scene</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/changan.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">DHGS:Decoupled Hybrid Gaussian Splatting for Driving Scene</h1>
          <div class="is-size-5 publication-authors">
            <style>
              .author-block a {
                text-decoration: none !important;
                color: inherit;
              }
            </style>
            <span class="author-block">
              <a href="javascript:void(0);">Xi Shi</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="javascript:void(0);">Lingli Chen</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="javascript:void(0);">Peng Wei</a><sup>*</sup>,
            </span>
            <span class="author-block">
              <a href="javascript:void(0);">Xi Wu</a><sup>&dagger;</sup>,
            </span>
            <span class="author-block">
              <a href="javascript:void(0);">Tian Jiang</a>,
            </span>
            <span class="author-block">
              <a href="javascript:void(0);">Yonggang Luo</a>,
            </span>
            <span class="author-block">
              <a href="javascript:void(0);">Lecheng Xie</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"> Changan Auto, AILab </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2407.16600"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block"> 
                <a href="https://github.com/Ironbrotherstyle/DHGS" 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/pipeline.png"
      class="interpolation-image"
      alt="Interpolate start reference image."/>
      </img>
      <h2 class="subtitle has-text-centered">
        The pipeline of the proposed method for driving scene reconstruction. Given consecutive multi-camera images along with their respective road and non-road masks,
         we initially generate decoupled road pcd (point cloud) and environment pcd, a road SDF is then pre-trained as subsequent guidance for the road Gaussian model. 
         The Environment pcd enables the initialization for the environment Gaussian model which composes rendered images with images by paratactic road model via the 
         proposed depth-ordered hybrid rendering.
      </h2>
    </div>
  </div>
</section>


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Existing Gaussian splatting methods often fall short in achieving satisfactory novel view synthesis in driving scenes, 
            primarily due to the absence of crafty design and geometric constraints for the involved elements. This paper introduces 
            a novel neural rendering method termed Decoupled Hybrid Gaussian Splatting (DHGS), targeting at promoting the rendering 
            quality of novel view synthesis for static driving scenes. The novelty of this work lies in the decoupled and hybrid pixel-level 
            blender for road and non-road layers, without the conventional unified differentiable rendering logic for the entire scene, while 
            still maintaining consistent and continuous superimposition through the proposed depth-ordered hybrid rendering strategy. Additionally, 
            an implicit road representation comprised of a Signed Distance Field (SDF) is trained to supervise the road surface with subtle geometric 
            attributes. Accompanied by the use of auxiliary transmittance loss and consistency loss, novel images with imperceptible boundary and 
            elevated fidelity are ultimately obtained. 
          </p> 

          <p>
            Substantial experiments on the Waymo dataset prove that DHGS outperforms the state-of-the-art methods.
          </p> 
          <!-- <p>
            We present the first method capable of photorealistically reconstructing a non-rigidly
            deforming scene using photos/videos captured casually from mobile phones.
          </p>
          <p>
            Our approach augments neural radiance fields
            (NeRF) by optimizing an
            additional continuous volumetric deformation field that warps each observed point into a
            canonical 5D NeRF.
            We observe that these NeRF-like deformation fields are prone to local minima, and
            propose a coarse-to-fine optimization method for coordinate-based models that allows for
            more robust optimization.
            By adapting principles from geometry processing and physical simulation to NeRF-like
            models, we propose an elastic regularization of the deformation field that further
            improves robustness.
          </p>
          <p>
            We show that <span class="dnerf">Nerfies</span> can turn casually captured selfie
            photos/videos into deformable NeRF
            models that allow for photorealistic renderings of the subject from arbitrary
            viewpoints, which we dub <i>"nerfies"</i>. We evaluate our method by collecting data
            using a
            rig with two mobile phones that take time-synchronized photos, yielding train/validation
            images of the same pose at different viewpoints. We show that our method faithfully
            reconstructs non-rigidly deforming scenes and reproduces unseen views with high
            fidelity.
          </p> -->
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <!-- <iframe src="https://www.youtube.com/watch?v=BLz4F7xo9ls"></iframe> -->
          <!-- <iframe src="https://www.youtube.com/embed/BLz4F7xo9ls?rel=0&amp;showinfo=0"
          frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe> -->
          <iframe src="https://player.bilibili.com/player.html?isOutside=true&aid=112844779555306&bvid=BV1V8e9eQEJU&cid=500001626559607&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-4">Pre-trained Surface Based On SDF</h2>

          <!-- <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/dollyzoom-stacked.mp4"
                    type="video/mp4">
          </video> -->
          <img src="./static/images/sdf_surface_compare.png"
          class="interpolation-image"
          alt="Interpolate start reference image."/>
          </img>
          <p>
            The left part shows the constraints guided by SDF. The top row (right part) displays rendered image images and ellipsoids without SDF regularizer,
             and the bottom row (right part) showcases results obtained with SDF regularizer. 
            It can be observed that the inclusion of SDF regularization leads the road model to render higher-quality images with the help of   better road geometry. 
          </p>
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-4">Depth-Ordered Hybrid Rendering</h2>
        <div class="columns is-centered">
          <div class="column content">

            <!-- <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video> -->
            <img src="./static/images/depth-order.png"
            class="interpolation-image"
            alt="Interpolate start reference image."/>
            </img>
            <p>
              The diagram illustrates the proposed depth-ordered hybrid rendering strategy for the environment and road model. 
              Corresponding primitives of each model generate pixels with colors independently through Gaussian splatting. 
              These colors are then composited based on their rendered depths and transmittances, producing the final rendered image.
            </p>
          </div>

        </div>
      </div>
    </div>
    <!--/ Matting. -->
  </section>

  <section class="section">
    <!-- Results-->
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Qualitative Experiment Results</h2>
          <div class="content has-text-centered">
            <img src="./static/images/train_test_2.png"
            class="interpolation-image"
            alt="Interpolate start reference image."/>
            </img>
            <p>
              Comparison of different methods on the Waymo dataset, the left column and right column display the quality of scene 
              reconstruction and novel view synthesis respectively. Our method achieves high-quality reconstruction for both the 
              environment region and the road areas, excelling over other comparative methods in these aspects.
            </p>
            <img src="./static/images/nvs_3.png"
            class="interpolation-image"
            alt="Interpolate start reference image."/>
            </img>
            <p>
              Visual comparisons on the free-view novel view synthesis. The left and right columns respectively exhibit the results 
              of Set3 and Set4 under the free viewpoint setting, where our method significantly outperforms other 
              comparative methods in capturing both road and environment details.
            </p>
            <!-- <div class="content has-text-justified">
              <p>
                Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
                viewpoint such as a stabilized camera by playing back the training deformations.
              </p>
            </div> -->
          </div>
        </div>
      </div>
    </div>
  </section>

    <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
            <h2 class="title is-3">Free-view novel view videos</h2>
            <!-- <div class="content has-text-justified">
              <p>
                Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
                viewpoint such as a stabilized camera by playing back the training deformations.
              </p>
            </div> -->
            <div class="content has-text-centered">
              <video id="replay-video"
                    controls
                    muted
                    preload
                    playsinline
                    autoplay
                    loop
                    width="100%">
                <source src="./static/videos/nvs3_demo.mp4"
                        type="video/mp4">
              </video>

              <video id="replay-video"
              controls
              muted
              preload
              playsinline
              autoplay
              loop
              width="100%">
          <source src="./static/videos/nvs4_demo.mp4"
                  type="video/mp4">
        </video>
            <video id="replay-video"
            controls
            muted
            preload
            playsinline
            autoplay
            loop
            width="100%">
        <source src="./static/videos/nvs_demo3_2.mp4"
                type="video/mp4">
      </video>

          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">


  <!-- cameras layout-->
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">The camera layouts of free-view novel view </h2>
        <div class="container is-max-desktop">
          <div class="content has-text-centered">
            <img src="./static/images/cameras.png"
            class="interpolation-image"
            alt="Interpolate start reference image."/>
            </img>
            <p>
              The top row represents the camera layouts after applying various camera pose settings from a bird's eye view, 
              while the bottom row shows the same transformations from a side view.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>





<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{shi2024dhgs,
  author    = {Xi, Shi and Lingli, Chen and Peng, Wei and Xi, Wu and Tian, Jiang and Yonggang, Luo and Lecheng, Xie},
  title     = {DHGS:Decoupled Hybrid Gaussian Splatting for Driving Scene},
  journal   = {arXiv preprint arXiv:2407.16600},
  year      = {2024},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2407.16600">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/Ironbrotherstyle/DHGS" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <!-- <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p> -->
          <p>
            This website is build based on <a
              href="https://github.com/nerfies/nerfies.github.io">nerfies</a>, we thank the author for providing template.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
